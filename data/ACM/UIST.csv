author,title,year,isbn,publisher,address,url,doi,abstract,booktitle,pages,numpages,keywords,location,series,articleno
"Khademi, Maryam and Fan, Mingming and Mousavi Hondori, Hossein and Lopes, Cristina Videira},","Multi-perspective multi-layer interaction on mobile device},","2013},","9781450324069},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/2508468.2514712},","10.1145/2508468.2514712},","We propose a novel multi-perspective multi-layer interaction using a mobile device, which provides an immersive experience of 3D navigation through an object. The mobile device serves as a window, through which the user can observe the object in detail from various perspectives by orienting the device differently. Various layers of the object can also be shown while users move the device away and toward themselves. Our approach is real-time, completely mobile (running on Android) and does not depend on external sensor/displays (e.g., camera and projector).},","Adjunct Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},","65–66},","2},","mobile, multi-layer interaction, multi-perspective, optical flow, spatially aware displays},","St. Andrews, Scotland, United Kingdom},",UIST '13 Adjunct,
"Yu, Zihao and Diakopoulos, Nicholas and Naaman, Mor},","The multiplayer: multi-perspective social video navigation},","2010},","9781450304627},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1866218.1866246},","10.1145/1866218.1866246},","We present a multi-perspective video ""multiplayer"" designed to organize social video aggregated from online sites like YouTube. Our system automatically time-aligns videos using audio fingerprinting, thus bringing them into a unified temporal frame. The interface utilizes social metadata to visually aid navigation and cue users to more interesting portions of an event. We provide details about the visual and interaction design rationale of the multiplayer.},","Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},","413–414},","2},","multi-perspective, social media, video},","New York, New York, USA},",UIST '10,
"Johns, Christoph Albert and Evangelista Belo, Jo\~{a}o Marcelo and Feit, Anna Maria and Klokmose, Clemens Nylandsted and Pfeuffer, Ken},","Towards Flexible and Robust User Interface Adaptations With Multiple Objectives},","2023},","9798400701320},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3586183.3606799},","10.1145/3586183.3606799},","This paper proposes a new approach for online UI adaptation that aims to overcome the limitations of the most commonly used UI optimization method involving multiple objectives: weighted sum optimization. Weighted sums are highly sensitive to objective formulation, limiting the effectiveness of UI adaptations. We propose ParetoAdapt, an adaptation approach that uses online multi-objective optimization with a posteriori articulated preferences—that is, articulation of preferences after the optimization has concluded—to make UI adaptation robust to incomplete and inaccurate objective formulations. It offers users a flexible way to control adaptations by selecting from a set of Pareto optimal adaptation proposals and adjusting them to fit their needs. We showcase the feasibility and flexibility of ParetoAdapt by implementing an online layout adaptation system in a state-of-the-art 3D UI adaptation framework. We further evaluate its robustness and run-time in simulation-based experiments that allow us to systematically change the accuracy of the estimated user preferences. We conclude by discussing how our approach may impact the usability and practicality of online UI adaptations.},","Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},","413–414},","17},","Pareto frontier, multi-objective optimization, online UI adaptation},","San Francisco, CA, USA},",UIST '23,"108},"
"Marquardt, Nicolai and Henry Riche, Nathalie and Holz, Christian and Romat, Hugo and Pahud, Michel and Brudy, Frederik and Ledo, David and Park, Chunjong and Nicholas, Molly Jane and Seyed, Teddy and Ofek, Eyal and Lee, Bongshin and Buxton, William A.S. and Hinckley, Ken},","AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures},","2021},","9781450386357},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3472749.3474820},","10.1145/3472749.3474820},","AirConstellations supports a unique semi-fixed style of cross-device interactions via multiple self-spatially-aware armatures to which users can easily attach (or detach) tablets and other devices. In particular, AirConstellations affords highly flexible and dynamic device formations where the users can bring multiple devices together in-air&nbsp;—&nbsp;with 2–5 armatures poseable in 7DoF within the same workspace&nbsp;—&nbsp;to suit the demands of their current task, social situation, app scenario, or mobility needs. This affords an interaction metaphor where relative orientation, proximity, attaching (or detaching) devices, and continuous movement into and out of ad-hoc ensembles can drive context-sensitive interactions. Yet all devices remain self-stable in useful configurations even when released in mid-air. We explore flexible physical arrangement, feedforward of transition options, and layering of devices in-air across a variety of multi-device app scenarios. These include video conferencing with flexible arrangement of the person-space of multiple remote participants around a shared task-space, layered and tiled device formations with overview+detail and shared-to-personal transitions, and flexible composition of UI panels and tool palettes across devices for productivity applications. A preliminary interview study highlights user reactions to AirConstellations, such as for minimally disruptive device formations, easier physical transitions, and balancing ”seeing and being seen” in remote work.},","The 34th Annual ACM Symposium on User Interface Software and Technology},","1252–1268},","17},","SurfaceFleet, cross-device computing, cross-device tracking, device ecologies, device formations, interaction techniques, multi-device, platform, semi-fixed workspaces},","Virtual Event, USA},",UIST '21,"108},"
"Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},","HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots},","2021},","9781450386357},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3472749.3474821},","10.1145/3472749.3474821},","HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},","The 34th Annual ACM Symposium on User Interface Software and Technology},","1269–1281},","13},","encountered-type haptics, swarm user interfaces, tabletop mobile robots, virtual reality},","Virtual Event, USA},",UIST '21,"108},"
"Nacenta, Miguel A. and Sakurai, Satoshi and Yamaguchi, Tokuo and Miki, Yohei and Itoh, Yuichi and Kitamura, Yoshifumi and Subramanian, Sriram and Gutwin, Carl},","E-conic: a perspective-aware interface for multi-display environments},","2007},","9781595936790},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1294211.1294260},","10.1145/1294211.1294260},","Multi-display environments compose displays that can be at different locations from and different angles to the user; as a result, it can become very difficult to manage windows, read text, and manipulate objects. We investigate the idea of perspective as a way to solve these problems in multi-display environments. We first identify basic display and control factors that are affected by perspective, such as visibility, fracture, and sharing. We then present the design and implementation of E-conic, a multi-display multi-user environment that uses location data about displays and users to dynamically correct perspective. We carried out a controlled experiment to test the benefits of perspective correction in basic interaction tasks like targeting, steering, aligning, pattern-matching and reading. Our results show that perspective correction significantly and substantially improves user performance in all these tasks.},","Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology},","279–288},","10},","legibility, multi-display environments, pattern matching, perspective-aware interfaces, steering, targeting},","Newport, Rhode Island, USA},",UIST '07,"108},"
"Sasaki, Masato and Nagamatsu, Takashi and Takemura, Kentaro},","Cross-Ratio Based Gaze Estimation for Multiple Displays using a Polarization Camera},","2019},","9781450368179},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3332167.3357095},","10.1145/3332167.3357095},","While eye tracking has been typically used for achieving intuitive user interfaces, it is not sufficient when it comes to dealing with multiple-display environments. In such environments, which have become popular recently, the point-of-gaze should be estimated on multiple screens. Therefore, we propose a cross-ratio based gaze estimation using a polarization camera for multiple screens. The point-of-gaze can be estimated on each monitor by identifying the screen reflected on the corneal surface at a polarization angle. Near-infrared light emitting diodes (NIR-LEDs) attached to the display are not required. This means that standard displays can be used with high general versatility as an advantage.},","Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},","1–3},","3},","gaze estimation, multiple displays, polarization},","New Orleans, LA, USA},",UIST '19 Adjunct,"108},"
"Tomohiro, Ayuri and Sumi, Yasuyuki},","Reconstruction of Scene from Multiple Sketches},","2016},","9781450345316},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/2984751.2985736},","10.1145/2984751.2985736},","This paper discusses the feasibility of extension of expressive style with multiple 3D sketches drawn by a sketching tool that enables its users to draw and paint on 3D structured surfaces. Users of our proposed system take a picture of target objects and sketch with reference to the taken picture. They can not only sketch on the pictures but can also change their viewpoint of the sketched environment, since the system captures 3D structure by using a depth sensor as well as RGB data. Trial usage of the system shows that our users can rapidly extract their target objects/space and extend their ideas by taking pictures and drawing/painting on them. This paper presents examples of system usage, and discusses the feasibility of extension of sketches.},","Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},","143–144},","2},","interactive illustrations, sketching},","Tokyo, Japan},",UIST '16 Adjunct,"108},"
"Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},","Demonstrating HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots},","2021},","9781450386555},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3474349.3480202},","10.1145/3474349.3480202},","HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},","Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology},","131–133},","3},","encountered-type haptics, swarm user interfaces, tabletop mobile robots, virtual reality},","Virtual Event, USA},",UIST '21 Adjunct,"108},"
"Shirai, Ryo and Itoh, Yuichi and Ueda, Shori and Onoye, Takao},","OptRod: Constructing Interactive Surface with Multiple Functions and Flexible Shape by Projected Image},","2018},","9781450359498},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3266037.3271639},","10.1145/3266037.3271639},","In this demonstration, we propose OptRod, constructing interactive surface with multiple functions and flexible shape by projected image. A PC generates images as control signals and projects them to the bottom of OptRods by a projector or LCD. An OptRod receives the light and converts its brightness into a control signal for the attached output device. By using multiple OptRods, the PC can simultaneously operate many output devices without any signal lines. Moreover, we can arrange surfaces of various shapes easily by combining multiple OptRods. OptRod supports various functions by replacing the device unit connected to OptRod.},","Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},","169–171},","3},","actuated surface, controlled by image, multi functional display, shape-free display, visible light communication},","Berlin, Germany},",UIST '18 Adjunct,"108},"
"Wilson, Andrew D. and Benko, Hrvoje},","Combining multiple depth cameras and projectors for interactions on, above and between surfaces},","2010},","9781450302715},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1866029.1866073},","10.1145/1866029.1866073},","Instrumented with multiple depth cameras and projectors, LightSpace is a small room installation designed to explore a variety of interactions and computational strategies related to interactive displays and the space that they inhabit. LightSpace cameras and projectors are calibrated to 3D real world coordinates, allowing for projection of graphics correctly onto any surface visible by both camera and projector. Selective projection of the depth camera data enables emulation of interactive displays on un-instrumented surfaces (such as a standard table or office desk), as well as facilitates mid-air interactions between and around these displays. For example, after performing multi-touch interactions on a virtual object on the tabletop, the user may transfer the object to another display by simultaneously touching the object and the destination display. Or the user may ""pick up"" the object by sweeping it into their hand, see it sitting in their hand as they walk over to an interactive wall display, and ""drop"" the object onto the wall by touching it with their other hand. We detail the interactions and algorithms unique to LightSpace, discuss some initial observations of use and suggest future directions.},","Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},","273–282},","10},","augmented reality, depth cameras, interactive spaces, surface computing, ubiquitous computing},","New York, New York, USA},",UIST '10,"108},"
"Pierce, Jeffrey S. and Nichols, Jeffrey},","An infrastructure for extending applications' user experiences across multiple personal devices},","2008},","9781595939753},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1449715.1449733},","10.1145/1449715.1449733},","Users increasingly interact with a heterogeneous collection of computing devices. The applications that users employ on those devices, however, still largely provide user experiences that assume the use of a single computer. This failure is due in part to the difficulty of creating user experiences that span multiple devices, particularly the need to manage identifying, connecting to, and communicating with other devices. In this paper we present an infrastructure based on instant messaging that simplifies adding that additional functionality to applications. Our infrastructure elevates device ownership to a first class property, allowing developers to provide functionality that spans personal devices without writing code to manage users' devices or establish connections among them. It also provides simple mechanisms for applications to send information, events, or commands between a user's devices. We demonstrate the effectiveness of our infrastructure by presenting a set of sample applications built with it and a user study demonstrating that developers new to the infrastructure can implement all of the cross-device functionality for three applications in, on average, less than two and a half hours.},","Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology},","101–110},","10},","application development, infrastructure, multi-device services, multi-device user experiences},","Monterey, CA, USA},",UIST '08,"108},"
"Sato, Toshiki and Koike, Hideki},","MlioLight: Multi-Layered Image Overlay using Multiple Flashlight Devices},","2016},","9781450345316},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/2984751.2985719},","10.1145/2984751.2985719},","We propose a technique that overlays natural images on the real world using the information from multiple flashlight devices. We focus on finding areas of overlapping lights in a multiple light-source scenario and overlaying multi-layered information on a real world object in these areas.In order to mix multiple images, we developed a light identification and overlapping area detection technique using rapid synchronization between high-speed cameras and multiple light devices.In this paper, we describe the concept of our system and a prototype implementation.We also describe two different applications.},","Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},","107–108},","2},","magic lens, multiple flashlights, projection mapping},","Tokyo, Japan},",UIST '16 Adjunct,"108},"
"Drucker, Steven M. and Petschnigg, Georg and Agrawala, Maneesh},","Comparing and managing multiple versions of slide presentations},","2006},","1595933131},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1166253.1166263},","10.1145/1166253.1166263},","Despite the ubiquity of slide presentations, managing multiple presentations remains a challenge. Understanding how multiple versions of a presentation are related to one another, assembling new presentations from existing presentations, and collaborating to create and edit presentations are difficult tasks. In this paper, we explore techniques for comparing and managing multiple slide presentations. We propose a general comparison framework for computing similarities and differences between slides. Based on this framework we develop an interactive tool for visually comparing multiple presentations. The interactive visualization facilitates understanding how presentations have evolved over time. We show how the interactive tool can be used to assemble new presentations from a collection of older ones and to merge changes from multiple presentation authors.},","Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology},","47–56},","10},","alignment, correspondence, distance metrics, slide presentations, versions},","Montreux, Switzerland},",UIST '06,"108},"
"Lander, Christian and Gehring, Sven and Kr\""{u}ger, Antonio and Boring, Sebastian and Bulling, Andreas},","GazeProjector: Accurate Gaze Estimation and Seamless Gaze Interaction Across Multiple Displays},","2015},","9781450337793},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/2807442.2807479},","10.1145/2807442.2807479},","Mobile gaze-based interaction with multiple displays may occur from arbitrary positions and orientations. However, maintaining high gaze estimation accuracy in such situa-tions remains a significant challenge. In this paper, we present GazeProjector, a system that combines (1) natural feature tracking on displays to determine the mobile eye tracker's position relative to a display with (2) accurate point-of-gaze estimation. GazeProjector allows for seam-less gaze estimation and interaction on multiple displays of arbitrary sizes independently of the user's position and orientation to the display. In a user study with 12 partici-pants we compare GazeProjector to established methods (here: visual on-screen markers and a state-of-the-art video-based motion capture system). We show that our approach is robust to varying head poses, orientations, and distances to the display, while still providing high gaze estimation accuracy across multiple displays without re-calibration for each variation. Our system represents an important step towards the vision of pervasive gaze-based interfaces.},","Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},","395–404},","10},","calibration, eye tracking, gaze estimation, large displays, multi-display environments, natural feature tracking},","Charlotte, NC, USA},",UIST '15,"108},"
"Hinckley, Ken},","Synchronous gestures for multiple persons and computers},","2003},","1581136366},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/964696.964713},","10.1145/964696.964713},","This research explores distributed sensing techniques for mobile devices using synchronous gestures. These are patterns of activity, contributed by multiple users (or one user with multiple devices), which take on a new meaning when they occur together in time, or in a specific sequence in time. To explore this new area of inquiry, this work uses tablet computers augmented with touch sensors and two-axis linear accelerometers (tilt sensors). The devices are connected via an 802.11 wireless network and synchronize their time-stamped sensor data. This paper describes a few practical examples of interaction techniques using synchronous gestures such as dynamically tiling together displays by physically bumping them together, discusses implementation issues, and speculates on further possibilities for synchronous gestures.},","Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},","149–158},","10},","context awareness, distributed sensor systems, input devices, multi-user interfaces, sensors, ubiquitous computing},","Vancouver, Canada},",UIST '03,"108},"
"Tsujii, Takahiro and Koizumi, Naoya and Naemura, Takeshi},","Inkantatory paper: dynamically color-changing prints with multiple functional inks},","2014},","9781450330688},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/2658779.2659103},","10.1145/2658779.2659103},","We propose an effective combination of multiple functional inks, including conductive silver ink, thermo-chromic ink, and regular inkjet ink, for a novel paper-based interface called Inkantatory Paper that can dynamically change the color of its printed pattern. Constructed with off-the-shelf inkjet printing using silver conductive ink, our system enables users to fabricate thin, flat, flexible, and low-cost interactive paper. We evaluated the characteristics of the conductive silver ink as a heating system for the thermo-chromic ink and created applications demonstrating the usability of the system.},","Adjunct Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},","39–40},","2},","conductive ink, digital fabrication, flexible printed electronics, inkjet printing, paper computing, prototyping},","Honolulu, Hawaii, USA},",UIST '14 Adjunct,"108},"
"Miller, Robert C. and Myers, Brad A.},","Synchronizing clipboards of multiple computers},","1999},","1581130759},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/320719.322584},","10.1145/320719.322584},","This paper describes a new technique for transferring data between computers, the synchronized clipboard. Multiple computers can share a synchronized clipboard for all clipboard operations, so that data copied to the clipboard from one computer, using the standard Copy command, can be pasted directly on another computer using the standard Paste command. Synchronized clipboards are well-suited for a single user moving data among several computers in close proximity. We describe an implementation of synchronized clipboards that works across a wide range of existing systems, including 3Com PalmPilots, Microsoft Windows PCs, Unix workstations, and other Java-capable platforms. Our implementation adds no noticeable overhead to local copy and paste operations.},","Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology},","65–66},","2},","Java, Pebbles, data transfer, distributed systems, drag-and-drop, file transfer, network clipboard, pick-and-drop, synchronized clipboard, ubiquitous computing},","Asheville, North Carolina, USA},",UIST '99,"108},"
"Yu, Chun and Shi, Yuanchun and Balakrishnan, Ravin and Meng, Xiangliang and Suo, Yue and Fan, Mingming and Qin, Yongqiang},","The satellite cursor: achieving MAGIC pointing without gaze tracking using multiple cursors},","2010},","9781450302715},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1866029.1866056},","10.1145/1866029.1866056},","We present the satellite cursor - a novel technique that uses multiple cursors to improve pointing performance by reducing input movement. The satellite cursor associates every target with a separate cursor in its vicinity for pointing, which realizes the MAGIC (manual and gaze input cascade) pointing method without gaze tracking. We discuss the problem of visual clutter caused by multiple cursors and propose several designs to mitigate it. Two controlled experiments were conducted to evaluate satellite cursor performance in a simple reciprocal pointing task and a complex task with multiple targets of varying layout densities. Results show the satellite cursor can save significant mouse movement and consequently pointing time, especially for sparse target layouts, and that satellite cursor performance can be accurately modeled by Fitts' Law.},","Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},","163–172},","10},","magic pointing, multiple cursor, reducing a},","New York, New York, USA},",UIST '10,"108},"
"Miyashita, Ken and Matsuoka, Satoshi and Takahashi, Shin and Yonezawa, Akinori},","Interactive generation of graphical user interfaces by multiple visual examples},","1994},","0897916573},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/192426.192462},","10.1145/192426.192462},","The construction of application-specific Graphical User Interfaces (GUI) still needs considerable programming partly because the mapping between application data and its visual representation is complicated. This study proposes a system which generates GUIs by generalizing multiple sets of application data and its visualization examples. The most notable characteristic of the system is that programmers can interactively modify the mapping by “correcting” the system-generated visualization examples that represent the system's current notion of programmer's intentions. Conflicting mappings are automatically resolved via the use of constraint hierarchies.},","Proceedings of the 7th Annual ACM Symposium on User Interface Software and Technology},","85–94},","10},","constraint hierarchies, graphical user interfaces, programming by example, visual parsing, visualization},","Marina del Rey, California, USA},",UIST '94,"108},"
"Nichols, Jeffrey and Rothrock, Brandon and Chau, Duen Horng and Myers, Brad A.},","Huddle: automatically generating interfaces for systems of multiple connected appliances},","2006},","1595933131},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1166253.1166298},","10.1145/1166253.1166298},","Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.},","Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology},","279–288},","10},","aggregate user interfaces, appliances, automatic interface generation, handheld computers, home theater, mobile phones, pebbles, personal digital assistants, personal universal controller (PUC)},","Montreux, Switzerland},",UIST '06,"108},"
"Vogel, Daniel and Balakrishnan, Ravin},","Interactive public ambient displays: transitioning from implicit to explicit, public to personal, interaction with multiple users},","2004},","1581139578},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/1029632.1029656},","10.1145/1029632.1029656},","We develop design principles and an interaction framework for sharable, interactive public ambient displays that support the transition from implicit to explicit interaction with both public and personal information. A prototype system implementation that embodies these design principles is described. We use novel display and interaction techniques such as simple hand gestures and touch screen input for explicit interaction and contextual body orientation and position cues for implicit interaction. Techniques are presented for subtle notification, self-revealing help, privacy controls, and shared use by multiple people each in their own context. Initial user feedback is also presented, and future directions discussed.},","Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},","137–146},","10},","ambient displays, interactive public displays, subtle interaction, ubicomp},","Santa Fe, NM, USA},",UIST '04,"108},"
"Dennler, Nathaniel Steele and Torrence, Evan and Yoo, Uksang and Nikolaidis, Stefanos and Mataric, Maja},","PyLips: an Open-Source Python Package to Expand Participation in Embodied Interaction},","2024},","9798400707186},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3672539.3686747},","10.1145/3672539.3686747},","We demonstrate PyLips, a Python package for expanding access to screen-based facial interfaces for text-to-speech. PyLips can be used to rapidly develop social interactions for a wide variety of applications. We designed PyLips to be easy to use for novice users and expressive for experienced interaction designers. We demonstrate key features of PyLips: compatibility across devices, customizable face appearance, and automated lip synching for text inputs. PyLips can be found at https://github.com/interaction-lab/PyLips.},","Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},","137–146},","4},","Conversational Agents, Human-Computer Interaction, Human-Robot Interaction},","Pittsburgh, PA, USA},",UIST Adjunct '24,"33},"
"Koyama, Kazuki and Narumi, Koya and Takaki, Ken and Kawase, Yasushi and Hautasaari, Ari and Kawahara, Yoshihiro},","Reusing Cardboard for Packaging Boxes with a Computational Design System},","2023},","9798400700965},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/3586182.3616692},","10.1145/3586182.3616692},","We propose a computational design system for packaging boxes, which enables reconstruction of cardboard into packaging structures that precisely fit the size of the objects to be shipped. Given a 3D model of the object and the structure of the cardboard to be reused, our system suggests the optimal net of a new box, which only requires minimum cuts and folds for fabrication by leveraging the edges of the input cardboard. To achieve this, we implemented a GUI visualizing the input models and the optimal net computed by our optimization algorithm. As a demonstration, we showed three design examples, with evaluation results of our method for reducing the length of cuts and folds.},","Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},","137–146},","3},","daily object reuse, optimization algorithm, personal fabrication, sharing economy, sustainable fabrication},","San Francisco, CA, USA},",UIST '23 Adjunct,"15},"
"Fogarty, James and Forlizzi, Jodi and Hudson, Scott E.},","Specifying behavior and semantic meaning in an unmodified layered drawing package},","2002},","1581134886},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/571985.571995},","10.1145/571985.571995},","In order to create and use rich custom appearances, designers are often forced to introduce an unnatural gap into the design process. For example, a designer creating a skin for a music player must separately specify the appearance of the elements in the music player skin and the mapping between these visual elements and the functionality provided by the music player. This gap between appearance and semantic meaning creates a number of problems. We present a set of techniques that allows designers to use their preferred drawing tool to specify both appearance and semantic meaning. We demonstrate our techniques in an unmodified version of Adobe Photoshop®, but our techniques are general and adaptable to nearly any layered drawing package.},","Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology},","61–70},","10},","prototyping, visual design tools, visual specification},","Paris, France},",UIST '02,"15},"
"Saakes, Daniel and Cambazard, Thomas and Mitani, Jun and Igarashi, Takeo},","PacCAM: material capture and interactive 2D packing for efficient material usage on CNC cutting machines},","2013},","9781450322683},","Association for Computing Machinery},","New York, NY, USA},","https://doi.org/10.1145/2501988.2501990},","10.1145/2501988.2501990},","The availability of low-cost digital fabrication devices enables new groups of users to participate in the design and fabrication of things. However, software to assist in the transition from design to actual fabrication is currently overlooked. In this paper, we introduce PacCAM, a system for packing 2D parts within a given source material for fabrication using 2D cutting machines. Our solution combines computer vision to capture the source material shape with a user interface that incorporates 2D rigid body simulation and snapping. A user study demonstrated that participants could make layouts faster with our system compared with using traditional drafting tools. PacCAM caters to a variety of 2D fabrication applications and can contribute to the reduction of material waste.},","Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},","441–446},","6},","2d shape packing, design tool, fabrication},","St. Andrews, Scotland, United Kingdom},",UIST '13,"15},"

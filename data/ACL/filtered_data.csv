,bib_url,ID,title,editor,month,year,address,publisher,url,author,booktitle,doi,pages,abstract,journal,volume,ISBN,language
257,/2020.coling-main.258.bib,258,"Computational Modeling of Affixoid Behavior in {C}hinese Morphology"",","Scott, Donia  and","dec,","2020"",","Barcelona, Spain (Online)"",","International Committee on Computational Linguistics"",","https://aclanthology.org/2020.coling-main.258/"",","Tseng, Yu-Hsiang  and","Proceedings of the 28th International Conference on Computational Linguistics"",","10.18653/v1/2020.coling-main.258"",",2879--2888,"The morphological status of affixes in Chinese has long been a matter of debate. How one might apply the conventional criteria of free/bound and content/function features to distinguish word-forming affixes from bound roots in Chinese is still far from clear. Issues involving polysemy and diachronic dynamics further blur the boundaries. In this paper, we propose three quantitative features in a computational model of affixoid behavior in Mandarin Chinese. The results show that, except for in a very few cases, there are no clear criteria that can be used to identify an affix’s status in an isolating language like Chinese. A diachronic check using contextualized embeddings with the WordNet Sense Inventory also demonstrates the possible role of the polysemy of lexical roots across diachronic settings.",,,,
632,/2020.conll-1.21.bib,21,"Modelling Lexical Ambiguity with Density Matrices"",","Fern{\'a}ndez, Raquel  and","nov,","2020"",","Online"",","Association for Computational Linguistics"",","https://aclanthology.org/2020.conll-1.21/"",","Meyer, Francois  and","Proceedings of the 24th Conference on Computational Natural Language Learning"",","10.18653/v1/2020.conll-1.21"",",276--290,"Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy. Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders.",,,,
695,/2020.tacl-1.31.bib,31,"A Neural Generative Model for Joint Learning Topics and Topic-Specific Word Embeddings"",","Johnson, Mark  and",,"2020"",","Cambridge, MA"",","MIT Press"",","https://aclanthology.org/2020.tacl-1.31/"",","Zhu, Lixing  and",,"10.1162/tacl_a_00326"",",471--485,"We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.","Transactions of the Association for Computational Linguistics"",","8"",",,
815,/2021.eacl-main.45.bib,45,"P}oly{LM}: Learning about Polysemy through Language Modeling"",","Merlo, Paola  and","apr,","2021"",","Online"",","Association for Computational Linguistics"",","https://aclanthology.org/2021.eacl-main.45/"",","Ansell, Alan  and","Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"",","10.18653/v1/2021.eacl-main.45"",",563--574,"To avoid the “meaning conflation deficiency” of word embeddings, a number of models have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses: firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on WSI, showing that it performs considerably better than previous sense embedding techniques, and matches the current state-of-the-art specialized WSI method despite having six times fewer parameters. Code and pre-trained models are available at https://github.com/AlanAnsell/PolyLM.",,,,
1067,/2021.eacl-main.297.bib,297,"Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings"",","Merlo, Paola  and","apr,","2021"",","Online"",","Association for Computational Linguistics"",","https://aclanthology.org/2021.eacl-main.297/"",","Xypolopoulos, Christos  and","Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"",","10.18653/v1/2021.eacl-main.297"",",3391--3401,"The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment .",,,,
1405,/2021.naacl-main.309.bib,309,"Multi-Hop Transformer for Document-Level Machine Translation"",","Toutanova, Kristina  and","jun,","2021"",","Online"",","Association for Computational Linguistics"",","https://aclanthology.org/2021.naacl-main.309/"",","Zhang, Long  and","Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",","10.18653/v1/2021.naacl-main.309"",",3953--3963,"Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior – human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.",,,,
1428,/2021.naacl-main.332.bib,332,"Multi-source Neural Topic Modeling in Multi-view Embedding Spaces"",","Toutanova, Kristina  and","jun,","2021"",","Online"",","Association for Computational Linguistics"",","https://aclanthology.org/2021.naacl-main.332/"",","Gupta, Pankaj  and","Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",","10.18653/v1/2021.naacl-main.332"",",4205--4217,"Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.",,,,
1623,/2021.tacl-1.50.bib,50,"Let{'}s Play Mono-Poly: {BERT} Can Reveal Words' Polysemy Level and Partitionability into Senses"",","Roark, Brian  and",,"2021"",","Cambridge, MA"",","MIT Press"",","https://aclanthology.org/2021.tacl-1.50/"",","Gar{\'i} Soler, Aina  and",,"10.1162/tacl_a_00400"",",825--844,"Pre-trained language models (LMs) encode rich information about linguistic structure but their knowledge about lexical polysemy remains unclear. We propose a novel experimental setup for analyzing this knowledge in LMs specifically trained for different languages (English, French, Spanish, and Greek) and in multilingual BERT. We perform our analysis on datasets carefully designed to reflect different sense distributions, and control for parameters that are highly correlated with polysemy such as frequency and grammatical category. We demonstrate that BERT-derived representations reflect words’ polysemy level and their partitionability into senses. Polysemy-related information is more clearly present in English BERT embeddings, but models in other languages also manage to establish relevant distinctions between words at different polysemy levels. Our results contribute to a better understanding of the knowledge encoded in contextualized representations and open up new avenues for multilingual lexical semantics research.","Transactions of the Association for Computational Linguistics"",","9"",",,
1654,/2021.tacl-1.81.bib,81,"A Biologically Plausible Parser"",","Roark, Brian  and",,"2021"",","Cambridge, MA"",","MIT Press"",","https://aclanthology.org/2021.tacl-1.81/"",","Mitropolsky, Daniel  and",,"10.1162/tacl_a_00432"",",1374--1388,"We describe a parser of English effectuated by biologically plausible neurons and synapses, and implemented through the Assembly Calculus, a recently proposed computational framework for cognitive function. We demonstrate that this device is capable of correctly parsing reasonably nontrivial sentences.1 While our experiments entail rather simple sentences in English, our results suggest that the parser can be extended beyond what we have implemented, to several directions encompassing much of language. For example, we present a simple Russian version of the parser, and discuss how to handle recursion, embedding, and polysemy.","Transactions of the Association for Computational Linguistics"",","9"",",,
1673,/2022.coling-1.7.bib,7,"Metaphorical Polysemy Detection: Conventional Metaphor Meets Word Sense Disambiguation"",","Calzolari, Nicoletta  and","oct,","2022"",","Gyeongju, Republic of Korea"",","International Committee on Computational Linguistics"",","https://aclanthology.org/2022.coling-1.7/"",","Maudslay, Rowan Hall  and","Proceedings of the 29th International Conference on Computational Linguistics"",",,65--77,"Linguists distinguish between novel and conventional metaphor, a distinction which the metaphor detection task in NLP does not take into account. Instead, metaphoricity is formulated as a property of a token in a sentence, regardless of metaphor type. In this paper, we investigate the limitations of treating conventional metaphors in this way, and advocate for an alternative which we name ‘metaphorical polysemy detection’ (MPD). In MPD, only conventional metaphoricity is treated, and it is formulated as a property of word senses in a lexicon. We develop the first MPD model, which learns to identify conventional metaphors in the English WordNet. To train it, we present a novel training procedure that combines metaphor detection with ‘word sense disambiguation’ (WSD). For evaluation, we manually annotate metaphor in two subsets of WordNet. Our model significantly outperforms a strong baseline based on a state-of-the-art metaphor detection model, attaining an ROC-AUC score of .78 (compared to .65) on one of the sets. Additionally, when paired with a WSD model, our approach outperforms a state-of-the-art metaphor detection model at identifying conventional metaphors in text (.659 F1 compared to .626).",,,,
1679,/2022.coling-1.13.bib,13,"Revisiting Statistical Laws of Semantic Shift in {R}omance Cognates"",","Calzolari, Nicoletta  and","oct,","2022"",","Gyeongju, Republic of Korea"",","International Committee on Computational Linguistics"",","https://aclanthology.org/2022.coling-1.13/"",","Kawasaki, Yoshifumi  and","Proceedings of the 29th International Conference on Computational Linguistics"",",,141--151,"This article revisits statistical relationships across Romance cognates between lexical semantic shift and six intra-linguistic variables, such as frequency and polysemy. Cognates are words that are derived from a common etymon, in this case, a Latin ancestor. Despite their shared etymology, some cognate pairs have experienced semantic shift. The degree of semantic shift is quantified using cosine distance between the cognates’ corresponding word embeddings. In the previous literature, frequency and polysemy have been reported to be correlated with semantic shift; however, the understanding of their effects needs revision because of various methodological defects. In the present study, we perform regression analysis under improved experimental conditions, and demonstrate a genuine negative effect of frequency and positive effect of polysemy on semantic shift. Furthermore, we reveal that morphologically complex etyma are more resistant to semantic shift and that the cognates that have been in use over a longer timespan are prone to greater shift in meaning. These findings add to our understanding of the historical process of semantic change.",,,,
2862,/2023.conll-1.13.bib,13,"Quirk or Palmer: A Comparative Study of Modal Verb Frameworks with Annotated Datasets"",","Jiang, Jing  and","dec,","2023"",","Singapore"",","Association for Computational Linguistics"",","https://aclanthology.org/2023.conll-1.13/"",","Owan, Risako  and","Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)"",","10.18653/v1/2023.conll-1.13"",",183--199,"Modal verbs, such as can, may, and must, are commonly used in daily communication to convey the speaker’s perspective related to the likelihood and/or mode of the proposition. They can differ greatly in meaning depending on how they’re used and the context of a sentence (e.g. “They must help each other out.” vs. “They must have helped each other out.”). Despite their practical importance in natural language understanding, linguists have yet to agree on a single, prominent framework for the categorization of modal verb senses. This lack of agreement stems from high degrees of flexibility and polysemy from the modal verbs, making it more difficult for researchers to incorporate insights from this family of words into their work. As a tool to help navigate this issue, this work presents MoVerb, a dataset consisting of 27,240 annotations of modal verb senses over 4,540 utterances containing one or more sentences from social conversations. Each utterance is annotated by three annotators using two different theoretical frameworks (i.e., Quirk and Palmer) of modal verb senses. We observe that both frameworks have similar inter-annotator agreements, despite having a different number of sense labels (eight for Quirk and three for Palmer). With RoBERTa-based classifiers fine-tuned on MoVerb, we achieve F1 scores of 82.2 and 78.3 on Quirk and Palmer, respectively, showing that modal verb sense disambiguation is not a trivial task.",,,,
2912,/2023.eacl-main.23.bib,23,"Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in {NLP}"",","Vlachos, Andreas  and","may,","2023"",","Dubrovnik, Croatia"",","Association for Computational Linguistics"",","https://aclanthology.org/2023.eacl-main.23/"",","Han, Xudong  and","Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",","10.18653/v1/2023.eacl-main.23"",",297--312,"Modern NLP systems exhibit a range of biases, which a growing literature on model debiasing attempts to correct. However, current progress is hampered by a plurality of definitions of bias, means of quantification, and oftentimes vague relation between debiasing algorithms and theoretical measures of bias. This paper seeks to clarify the current situation and plot a course for meaningful progress in fair learning, with two key contributions: (1) making clear inter-relations among the current gamut of methods, and their relation to fairness theory; and (2) addressing the practical problem of model selection, which involves a trade-off between fairness and accuracy and has led to systemic issues in fairness research. Putting them together, we make several recommendations to help shape future work.",,,,
2947,/2023.eacl-main.58.bib,58,"The Functional Relevance of Probed Information: A Case Study"",","Vlachos, Andreas  and","may,","2023"",","Dubrovnik, Croatia"",","Association for Computational Linguistics"",","https://aclanthology.org/2023.eacl-main.58/"",","Hanna, Michael  and","Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",","10.18653/v1/2023.eacl-main.58"",",835--848,"Recent studies have shown that transformer models like BERT rely on number information encoded in their representations of sentences’ subjects and head verbs when performing subject-verb agreement. However, probing experiments suggest that subject number is also encoded in the representations of all words in such sentences. In this paper, we use causal interventions to show that BERT only uses the subject plurality information encoded in its representations of the subject and words that agree with it in number. We also demonstrate that current probing metrics are unable to determine which words’ representations contain functionally relevant information. This both provides a revised view of subject-verb agreement in language models, and suggests potential pitfalls for current probe usage and evaluation.",,,,
3056,/2023.eacl-main.167.bib,167,"Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks"",","Vlachos, Andreas  and","may,","2023"",","Dubrovnik, Croatia"",","Association for Computational Linguistics"",","https://aclanthology.org/2023.eacl-main.167/"",","Renner, Joseph  and","Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",","10.18653/v1/2023.eacl-main.167"",",2277--2290,"The psychological plausibility of word embeddings has been studied through different tasks such as word similarity, semantic priming, and lexical entailment. Recent work on predicting category structure with word embeddings report low correlations with human ratings. (Heyman and Heyman, 2019) showed that static word embeddings fail at predicting typicality using cosine similarity between category and exemplar words, while (Misra et al., 2021)obtain equally modest results for various contextual language models (CLMs) using a Cloze task formulation over hand-crafted taxonomic sentences. In this work, we test a wider array of methods for probing CLMs for predicting typicality scores. Our experiments, using BERT (Devlin et al., 2018), show the importance of using the right type of CLM probes, as our best BERT-based typicality prediction methods improve on previous works. Second, our results highlight the importance of polysemy in this task, as our best results are obtained when contextualization is paired with a disambiguation mechanism as in (Chronis and Erk, 2020). Finally, additional experiments and analyses reveal that Information Content-based WordNet (Miller, 1995) similarities with disambiguation match the performance of the best BERT-based method, and in fact capture complementary information, and when combined with BERT allow for enhanced typicality predictions.",,,,
3397,/2024.eacl-long.90.bib,90,"3{D} Rotation and Translation for Hyperbolic Knowledge Graph Embedding"",","Graham, Yvette  and","mar,","2024"",","St. Julian{'}s, Malta"",","Association for Computational Linguistics"",","https://aclanthology.org/2024.eacl-long.90/"",","Zhu, Yihua  and","Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)"",","10.18653/v1/2024.eacl-long.90"",",1497--1515,"The main objective of Knowledge Graph (KG) embeddings is to learn low-dimensional representations of entities and relations, enabling the prediction of missing facts. A significant challenge in achieving better KG embeddings lies in capturing relation patterns, including symmetry, antisymmetry, inversion, commutative composition, non-commutative composition, hierarchy, and multiplicity. This study introduces a novel model called 3H-TH (3D Rotation and Translation in Hyperbolic space) that captures these relation patterns simultaneously. In contrast, previous attempts have not achieved satisfactory performance across all the mentioned properties at the same time. The experimental results demonstrate that the new model outperforms existing state-of-the-art models in terms of accuracy, hierarchy property, and other relation patterns in low-dimensional space, meanwhile performing similarly in high-dimensional space.",,,,
3813,/2024.lrec-main.325.bib,325,"Computational Modelling of Plurality and Definiteness in {C}hinese Noun Phrases"",","Calzolari, Nicoletta  and","may,","2024"",","Torino, Italia"",","ELRA and ICCL"",","https://aclanthology.org/2024.lrec-main.325/"",","Liu, Yuqi  and","Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"",",,3666--3676,"Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are “cooler” than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using both classic machine learning models and state-of-the-art pre-trained language models to predict the plurality and definiteness of each NP. We report on the performance of these models and analyse their behaviours.",,,,
4820,/2024.lrec-main.1332.bib,1332,"Strengthening the {W}i{C}: New Polysemy Dataset in {H}indi and Lack of Cross Lingual Transfer"",","Calzolari, Nicoletta  and","may,","2024"",","Torino, Italia"",","ELRA and ICCL"",","https://aclanthology.org/2024.lrec-main.1332/"",","Dubossarsky, Haim  and","Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"",",,15341--15349,"This study addresses the critical issue of Natural Language Processing in low-resource languages such as Hindi, which, despite having substantial number of speakers, is limited in linguistic resources. The paper focuses on Word Sense Disambiguation, a fundamental NLP task that deals with polysemous words. It introduces a novel Hindi WSD dataset in the modern WiC format, enabling the training and testing of contextualized models. The primary contributions of this work lie in testing the efficacy of multilingual models to transfer across languages and hence to handle polysemy in low-resource languages, and in providing insights into the minimum training data required for a viable solution. Experiments compare different contextualized models on the WiC task via transfer learning from English to Hindi. Models purely transferred from English yield poor 55% accuracy, while fine-tuning on Hindi dramatically improves performance to 90% accuracy. This demonstrates the need for language-specific tuning and resources like the introduced Hindi WiC dataset to drive advances in Hindi NLP. The findings offer valuable insights into addressing the NLP needs of widely spoken yet low-resourced languages, shedding light on the problem of transfer learning in these contexts.",,,,
5597,/2024.tacl-1.68.bib,68,"Do Vision and Language Models Share Concepts? A Vector Space Alignment Study"",",,,"2024"",","Cambridge, MA"",","MIT Press"",","https://aclanthology.org/2024.tacl-1.68/"",","Li, Jiaang  and",,"10.1162/tacl_a_00698"",",1232--1249,"Large-scale pretrained language models (LMs) are said to “lack the ability to connect utterances to the world” (Bender and Koller, 2020), because they do not have “mental models of the world” (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. We present an empirical evaluation across four families of LMs (BERT, GPT-2, OPT, and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy, and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).1","Transactions of the Association for Computational Linguistics"",","12"",",,
5729,/2025.coling-main.105.bib,105,"Polysemy Interpretation and Transformer Language Models: A Case of {K}orean Adverbial Postposition -(u)lo"",","Rambow, Owen  and","jan,","2025"",","Abu Dhabi, UAE"",","Association for Computational Linguistics"",","https://aclanthology.org/2025.coling-main.105/"",","Mun, Seongmin  and","Proceedings of the 31st International Conference on Computational Linguistics"",",,1555--1561,"This study examines how Transformer language models utilise lexico-phrasal information to interpret the polysemy of the Korean adverbial postposition -(u)lo. We analysed the attention weights of both a Korean pre-trained BERT model and a fine-tuned version. Results show a general reduction in attention weights following fine-tuning, alongside changes in the lexico-phrasal information used, depending on the specific function of -(u)lo. These findings suggest that, while fine-tuning broadly affects a model’s syntactic sensitivity, it may also alter its capacity to leverage lexico-phrasal features according to the function of the target word.",,,,
6376,/2025.coling-main.752.bib,752,"PERSONA}: A Reproducible Testbed for Pluralistic Alignment"",","Rambow, Owen  and","jan,","2025"",","Abu Dhabi, UAE"",","Association for Computational Linguistics"",","https://aclanthology.org/2025.coling-main.752/"",","Castricato, Louis  and","Proceedings of the 31st International Conference on Computational Linguistics"",",,11348--11368,"The rapid advancement of language models (LMs) necessitates robust alignment with diverse user values. However, current preference optimization approaches often fail to capture the plurality of user opinions, instead reinforcing majority viewpoints and marginalizing minority perspectives. We introduce PERSONA, a reproducible test bed designed to evaluate and improve pluralistic alignment of LMs. We procedurally generate diverse user profiles from US census data, resulting in 1,586 synthetic personas with varied demographic and idiosyncratic attributes. We then generate a large-scale evaluation dataset containing 3,868 prompts and 317,200 feedback pairs obtained from our synthetic personas. Leveraging this dataset, we systematically evaluate LM capabilities in role-playing diverse users, verified through human judges, and the establishment of both a benchmark, PERSONA Bench, for pluralistic alignment approaches as well as an extensive dataset to create new and future benchmarks.",,,,
8329,/C16-1073.bib,1073,"Different Contexts Lead to Different Word Embeddings"",","Matsumoto, Yuji  and","dec,","2016"",","Osaka, Japan"",","The COLING 2016 Organizing Committee"",","https://aclanthology.org/C16-1073/"",","Hu, Wenpeng  and","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers"",",,762--771,"Recent work for learning word representations has applied successfully to many NLP applications, such as sentiment analysis and question answering. However, most of these models assume a single vector per word type without considering polysemy and homonymy. In this paper, we present an extension to the CBOW model which not only improves the quality of embeddings but also makes embeddings suitable for polysemy. It differs from most of the related work in that it learns one semantic center embedding and one context bias instead of training multiple embeddings per word type. Different context leads to different bias which is defined as the weighted average embeddings of local context. Experimental results on similarity task and analogy task show that the word representations learned by the proposed method outperform the competitive baselines.",,,,
8491,/C16-1235.bib,1235,"Distance Metric Learning for Aspect Phrase Grouping"",","Matsumoto, Yuji  and","dec,","2016"",","Osaka, Japan"",","The COLING 2016 Organizing Committee"",","https://aclanthology.org/C16-1235/"",","Xiong, Shufeng  and","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers"",",,2492--2502,"Aspect phrase grouping is an important task in aspect-level sentiment analysis. It is a challenging problem due to polysemy and context dependency. We propose an Attention-based Deep Distance Metric Learning (ADDML) method, by considering aspect phrase representation as well as context representation. First, leveraging the characteristics of the review text, we automatically generate aspect phrase sample pairs for distant supervision. Second, we feed word embeddings of aspect phrases and their contexts into an attention-based neural network to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace for measure the distances between aspect phrases for K-means clustering. Experiments on four review datasets show that the proposed method outperforms state-of-the-art strong baseline methods.",,,,
10081,/C94-2110.bib,2110,"Virtual Polysemy"",",,,"1994"",",,,https://aclanthology.org/C94-2110/,"Sanfilippo, Antonio  and","COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics"",",,,,,,,
10093,/C94-2122.bib,2122,"Automatic Recognition of Verbal Polysemy"",",,,"1994"",",,,https://aclanthology.org/C94-2122/,"Fukumoto, Fumiyo  and","COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics"",",,,,,,,
10348,/C96-2163.bib,2163,"Sense Classification of Verbal Polysemy based-on Bilingual Class/Class Association"",",,,"1996"",",,,https://aclanthology.org/C96-2163/,"Utsuro, Takehito"",","COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics"",",,,,,,,
10667,/D07-1018.bib,1018,"Modelling Polysemy in Adjective Classes by Multi-Label Classification"",","Eisner, Jason"",","jun,","2007"",","Prague, Czech Republic"",","Association for Computational Linguistics"",","https://aclanthology.org/D07-1018/"",","Boleda, Gemma  and","Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})"",",,171--180,,,,,
11344,/E85-1003.bib,1003,"Distributives, Quantifiers and a Multiplicity of Events"",","King, Maghi"",","mar,","1985"",","Geneva, Switzerland"",","Association for Computational Linguistics"",",https://aclanthology.org/E85-1003/,"Stirling, Lesley"",","Second Conference of the {E}uropean Chapter of the Association for Computational Linguistics"",",,,,,,,
11981,/N01-1009.bib,1009,"A Corpus-based Account of Regular Polysemy: The Case of Context-sensitive Adjectives"",",,,"2001"",",,,https://aclanthology.org/N01-1009/,"Lapata, Maria"",","Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics"",",,,,,,,
11982,/N01-1010.bib,1010,"Tree-Cut and a Lexicon Based on Systematic Polysemy"",",,,"2001"",",,,https://aclanthology.org/N01-1010/,"Tomuro, Noriko"",","Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics"",",,,,,,,
13254,/N18-1202.bib,1202,"Deep Contextualized Word Representations"",","Walker, Marilyn  and","jun,","2018"",","New Orleans, Louisiana"",","Association for Computational Linguistics"",","https://aclanthology.org/N18-1202/"",","Peters, Matthew E.  and","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"",","10.18653/v1/N18-1202"",",2227--2237,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",,,,
13474,/N19-1217.bib,1217,"Joint Detection and Location of {E}nglish Puns"",","Burstein, Jill  and","jun,","2019"",","Minneapolis, Minnesota"",","Association for Computational Linguistics"",","https://aclanthology.org/N19-1217/"",","Zou, Yanyan  and","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"",","10.18653/v1/N19-1217"",",2117--2123,"A pun is a form of wordplay for an intended humorous or rhetorical effect, where a word suggests two or more meanings by exploiting polysemy (homographic pun) or phonological similarity to another word (heterographic pun). This paper presents an approach that addresses pun detection and pun location jointly from a sequence labeling perspective. We employ a new tagging scheme such that the model is capable of performing such a joint task, where useful structural information can be properly captured. We show that our proposed model is effective in handling both homographic and heterographic puns. Empirical results on the benchmark datasets demonstrate that our approach can achieve new state-of-the-art results.",,,,
13956,/P97-1010.bib,1010,"Homonymy and Polysemy in Information Retrieval"",",,"jul,","1997"",","Madrid, Spain"",","Association for Computational Linguistics"",","https://aclanthology.org/P97-1010/"",","Krovetz, Robert"",","35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics"",","10.3115/976909.979627"",",72--79,,,,,
14249,/Q18-1034.bib,1034,"Linear Algebraic Structure of Word Senses, with Applications to Polysemy"",","Lee, Lillian  and",,"2018"",","Cambridge, MA"",","MIT Press"",","https://aclanthology.org/Q18-1034/"",","Arora, Sanjeev  and",,"10.1162/tacl_a_00034"",",483--495,"Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 “discourse atoms” that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.","Transactions of the Association for Computational Linguistics"",","6"",",,
14251,/Q18-1036.bib,1036,"Neural Lattice Language Models"",","Lee, Lillian  and",,"2018"",","Cambridge, MA"",","MIT Press"",","https://aclanthology.org/Q18-1036/"",","Buckman, Jacob  and",,"10.1162/tacl_a_00036"",",529--541,"In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions — including polysemy and the existence of multiword lexical items — into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.","Transactions of the Association for Computational Linguistics"",","6"",",,
